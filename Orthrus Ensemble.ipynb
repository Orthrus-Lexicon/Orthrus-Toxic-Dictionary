{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load training data\n",
      "Load trial data\n",
      "Load test data\n",
      "Prepare training data\n",
      "Training phase\n",
      "Losses {'ner': 31736.713823175938}\n",
      "\n",
      "Evaluation test\n",
      "Average F1 Ensemble Intersection 0.633024\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import csv\n",
    "import random\n",
    "import statistics\n",
    "import sys\n",
    "\n",
    "import sklearn\n",
    "import spacy\n",
    "\n",
    "import semeval2021\n",
    "import fix_spans\n",
    "\n",
    "import spans_detection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def spans_to_ents(doc, spans, label):\n",
    "  \"\"\"Converts span indicies into spacy entity labels.\"\"\"\n",
    "  started = False\n",
    "  left, right, ents = 0, 0, []\n",
    "  for x in doc:\n",
    "    if x.pos_ == 'SPACE':\n",
    "      continue\n",
    "    if spans.intersection(set(range(x.idx, x.idx + len(x.text)))):\n",
    "      if not started:\n",
    "        left, started = x.idx, True\n",
    "      right = x.idx + len(x.text)\n",
    "    elif started:\n",
    "      ents.append((left, right, label))\n",
    "      started = False\n",
    "  if started:\n",
    "    ents.append((left, right, label))\n",
    "  return ents\n",
    "\n",
    "\n",
    "def read_datafile(filename):\n",
    "  \"\"\"Reads csv file with python span list and text.\"\"\"\n",
    "  data = []\n",
    "  with open(filename, encoding='utf8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    count = 0\n",
    "    for row in reader:\n",
    "      fixed = fix_spans.fix_spans(\n",
    "          ast.literal_eval(row['spans']), row['text'])\n",
    "      data.append((fixed, row['text']))\n",
    "  return data\n",
    "\n",
    "def evaluate(dataset, toxic_list, model):\n",
    "  test_texts = []\n",
    "  for spans, text in dataset:\n",
    "    test_texts.append(text)\n",
    "        \n",
    "  detected_spans = spans_detection.get_spans_from_dataset(test_texts, toxic_list)\n",
    "  i = 0\n",
    "    \n",
    "  intersection_scores = []\n",
    "    \n",
    "  for spans, text in dataset:\n",
    "    pred_spans = []\n",
    "    intersection_spans = []\n",
    "    doc = model(text)\n",
    "    for ent in doc.ents:\n",
    "      pred_spans.extend(range(ent.start_char, ent.start_char + len(ent.text)))\n",
    "      intersection_spans.extend(range(ent.start_char, ent.start_char + len(ent.text)))\n",
    "\n",
    "    # Get Ensemble intersection score\n",
    "    intersection = set(intersection_spans).intersection(set(detected_spans[i]))\n",
    "    intersection = sorted(list(intersection))\n",
    "    score = semeval2021.f1(intersection, spans)\n",
    "    intersection_scores.append(score)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "  print('Average F1 Ensemble Intersection %g' % statistics.mean(intersection_scores))\n",
    "\n",
    "def main():\n",
    "  \"\"\"Train and eval a spacy named entity tagger for toxic spans.\"\"\"\n",
    "  # Read training data\n",
    "  print('Load training data')\n",
    "  train = read_datafile('./data/tsd_train.csv')\n",
    "    \n",
    "  # Read trial data\n",
    "  print('Load trial data')\n",
    "  trial = read_datafile('./data/tsd_trial.csv')\n",
    "\n",
    "  # Read trial data for test.\n",
    "  print('Load test data')\n",
    "  test = read_datafile('./data/tsd_test_with_ground_truth.csv')\n",
    "\n",
    "  # Get the toxic word list\n",
    "  file = open('Toxic words dictionary.txt', 'r', encoding=\"ISO-8859-1\")\n",
    "  content = file.read()\n",
    "  file.close()\n",
    "  toxic_list = content.split('\\n')\n",
    "  unique_toxic_list = np.unique(toxic_list)\n",
    "\n",
    "  # Convert training data to Spacy Entities\n",
    "  nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "  print('Prepare training data')\n",
    "  training_data = []\n",
    "  for n, (spans, text) in enumerate(train):\n",
    "    doc = nlp(text)\n",
    "    ents = spans_to_ents(doc, set(spans), 'TOXIC')\n",
    "    training_data.append((doc.text, {'entities': ents}))\n",
    "\n",
    "  toxic_tagging = spacy.blank('en')\n",
    "  toxic_tagging.vocab.strings.add('TOXIC')\n",
    "  ner = nlp.create_pipe(\"ner\")\n",
    "  toxic_tagging.add_pipe(ner, last=True)\n",
    "  ner.add_label('TOXIC')\n",
    "\n",
    "  pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "  unaffected_pipes = [\n",
    "      pipe for pipe in toxic_tagging.pipe_names\n",
    "      if pipe not in pipe_exceptions]\n",
    "\n",
    "  print('Training phase')\n",
    "  with toxic_tagging.disable_pipes(*unaffected_pipes):\n",
    "    toxic_tagging.begin_training()\n",
    "    for iteration in range(25):\n",
    "      random.shuffle(training_data)\n",
    "      losses = {}\n",
    "      batches = spacy.util.minibatch(\n",
    "          training_data, size=spacy.util.compounding(\n",
    "              4.0, 32.0, 1.001))\n",
    "      for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        toxic_tagging.update(texts, annotations, drop=0.5, losses=losses)\n",
    "      print(\"Losses\", losses)\n",
    "\n",
    "  # Evaluation //////////////////////////////   \n",
    "  print('')\n",
    "  print('Evaluation of the test data')\n",
    "  evaluate(test, unique_toxic_list, toxic_tagging)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
